{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8269f9cb-4fda-406a-b8c7-5ec5a03d7bc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install Required Python Libraries\n",
    "First, let's install the required Python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8969d02e-202a-412e-bad8-efb1e7f847f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -U llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b982b5f0-d6c2-45db-93a9-8107d4940162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow>=3.0 databricks-feature-engineering --upgrade\n",
    "%pip install llama-index-llms-databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef61d6d-627f-4b74-b364-92602e8c2895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f750771-ac0d-46d6-b20b-8865be4abe19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, let's explore the PDF content we parsed into structured format in our knowledge base table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c1dd5a-ca9a-4ad7-ba65-d2e68942a721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- select * from <CATALOG_NAME>.<SCHEMA_NAME>.parsed_policy_pdfs;\n",
    "select * from hytech_workshop.ai_agent.parsed_policy_pdfs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7d6cde-0cd5-4dd2-810e-795b59351008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdfs_df = spark.table(\"hytech_workshop.ai_agent.parsed_policy_pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306c8232-dfc0-4793-a6ad-8a9d4f38481a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.llms.databricks import Databricks\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Chunking UDF using Databricks LLM Model\n",
    "@pandas_udf(\"array<string>\")\n",
    "def chunk_pdf_content(content: pd.Series) -> pd.Series:\n",
    "    llm = Databricks(\n",
    "        model=\"databricks-gte-large-en\",\n",
    "        api_key=\"<INSERT YOUR PERSONAL ACCESS TOKEN HERE>\",\n",
    "        api_base=\"https://<DATABRICKS WORKSPACE URL>/serving-endpoints\"\n",
    "    )    \n",
    "    Settings.llm = llm\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "    def extract_and_split(txt):\n",
    "        nodes = splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "        return [n.text for n in nodes]\n",
    "\n",
    "    return content.apply(extract_and_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b205fb-53ff-4898-a15e-ace566ba9960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Chunk using Databricks LLM Models\n",
    "exploded_chunks_dbx_df = (pdfs_df\n",
    "                .withColumn(\"content\", explode(chunk_pdf_content(\"content\")))\n",
    "                 .selectExpr('doc_uri as pdf_name', 'content')\n",
    "                )\n",
    "display(exploded_chunks_dbx_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc521de4-e00c-46b6-9e5d-9ba12217364e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Save Chunked Data as a Vector Database Table\n",
    "\n",
    "This cell saves the exploded chunks DataFrame as a Delta Table, enabling efficient semantic search and retrieval over your PDF documents using vector embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53801bd1-2c72-419e-af7b-90971609a8ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'CATALOG_NAME'\n",
    "schema = 'SCHEMA_NAME'\n",
    "catalog = 'hytech_workshop'\n",
    "schema = 'ai_agent'\n",
    "\n",
    "exploded_chunks_dbx_df.write.mode('overwrite').option(\"delta.enableChangeDataFeed\", \"true\").saveAsTable(f\"{catalog}.{schema}.policy_pdfs_chunked_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e1cc59-2580-46d1-bf0a-b366497c1b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Vector Search Index for `policy_pdfs_chunked_db`\n",
    "\n",
    "To enable efficient semantic search over your PDF chunks, you need to create a vector search index on your Delta table (`policy_pdfs_chunked_db`). Follow these steps:\n",
    "\n",
    "1. **Ensure Requirements:**\n",
    "   - Your workspace must have Unity Catalog enabled.\n",
    "   - Serverless compute must be enabled.\n",
    "   - You must have `CREATE TABLE` privileges on the target schema.\n",
    "\n",
    "2. **Create the Vector Search Index:**\n",
    "   - You can use the Databricks UI, Python SDK, or REST API. The UI is the simplest method.\n",
    "\n",
    "   **Using the Databricks UI:**\n",
    "   - Go to the Data tab in your Databricks workspace.\n",
    "   - Locate your Delta table (`policy_pdfs_chunked_db`).\n",
    "   - Click on the table and select \"Create Vector Search Index\".\n",
    "   - Follow the prompts to configure the index (choose the embedding column, set index options, etc.).\n",
    "   - Click \"Create\".\n",
    "\n",
    "   **Using Python SDK or REST API:**\n",
    "   - Refer to the [Databricks documentation](https://learn.microsoft.com/en-us/azure/databricks/vector-search/create-vector-search/) for code examples and API details.\n",
    "\n",
    "3. **After Creation:**\n",
    "   - The index will automatically sync with your Delta table.\n",
    "   - You can now perform semantic search queries using the vector search endpoint.\n",
    "\n",
    "> **Note:** Do not use a column named `_id` in your source table, as it is reserved for vector search indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32706db-a87b-4d43-836f-3d043dca795a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6441538139443277,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Prepare Data for RAG",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
